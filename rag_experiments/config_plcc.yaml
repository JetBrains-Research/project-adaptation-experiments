output:
  result_folder: /mnt/data2/galimzyanov/long-contex-eval/output/rag/dev/
  results_filename: "results_dev.jsonl"
data:
  dataset_name: "JetBrains-Research/lca-project-level-code-completion"
  composer_name: "path_distance"
  # Eligible scopes: ["medium_context", "large_context", "huge_context"]
  # do not use "small_context"
  context_scopes: ["medium_context"]
  completion_categories: ["infile", "inproject"]
  filter_extensions: true
  # Only those extentions would be added to the context
  lang_extensions: [".py"]
  allowed_extensions: [".md", ".txt", ".rst"]
model:
  # "codellama/CodeLlama-7b-hf", "deepseek-ai/deepseek-coder-1.3b-base"
  # "codellama/CodeLlama-7b-Python-hf"
  model_name: "deepseek-ai/deepseek-coder-1.3b-base"
  # We assess the avg toke length to make context truncation more effective.
  # We truncate the context before tokenization. Tokenizer truncates se seauence too.
  # Here instead you can fix it by any float.
  # If it is too large, no problem, the context would not be truncated before tokenization.
  token_length: 4
  tok_len_asses_cap: 2000000
use_vllm: True
vllm:
  vllm_args:
    download_dir: /mnt/data2/tmp
#    gpu_memory_utilization: 0.1
  generation_args:
    temperature: 0.0
    max_tokens: 100
    # Used to avoid early stopping on the empty line
    min_tokens: 5
    stop: ["\n"]
eval:
  # For VLLM this value can be arbitrary large, since it batches the items inside
  batch_size: 20
  context_size: 16256 # 16256
  # Is not used in the library, but can be used in outer scripts to iterate over the context.
  context_size_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16256] # 16256 = 16384 - 128
  max_new_tokens: 128