output:
  result_folder: "/mnt/data/kolomyttseva/long-contex-eval/output/rag/"
  results_filename: "results_draco.jsonl"
data:
  language: "python" # "python", "kotlin"
  dataset_name: null
  # composer_name ["path_distance", "iou_chunk_score", "iou_file_score", "from_file", "no_context", "kl_chunk_score", "draco"]
  composer_name: "chunk_score"
  topk: -1
  # composer_dataset_file: "/mnt/data/galimzyanov/long-contex-eval/datasets/plcc_optimal_medium_unique.jsonl"
  # composer_dataset_file: "/mnt/data/galimzyanov/long-contex-eval/datasets/plcc_medium_pathdist_olga.jsonl"
  # Eligible scopes: ["medium_context", "large_context", "huge_context"]
  # do not use "small_context"
  context_scopes: ["medium_context"]
  completion_categories: ["infile", "inproject"] #"infile", "inproject"
  # time_scopes: ["after_year2022", "before_year2022"]
  filter_extensions: true
  # Only those extentions would be added to the context
  allowed_extensions: [".md", ".txt", ".rst"]
  lang_extensions: null
model:
  # "codellama/CodeLlama-7b-hf", "deepseek-ai/deepseek-coder-1.3b-base"
  # "codellama/CodeLlama-7b-Python-hf"
  model_name_or_path: "deepseek-ai/deepseek-coder-1.3b-base"
  # We assess the avg toke length to make context truncation more effective.
  # We truncate the context before tokenization. Tokenizer truncates se sequence too.
  # Here instead you can fix it by any float.
  # If it is too large, no problem, the context would not be truncated before tokenization.
  token_length: 4
  tok_len_asses_cap: 2000000
use_vllm: True
vllm:
  vllm_args:
     # max_model_len: 512
    #  gpu_memory_utilization: 0.15
    max_seq_len_to_capture: 16400
  generation_args:
    temperature: 0.0
    max_tokens: 100
    # Used to avoid early stopping on the empty line
    min_tokens: 5
    stop: ["\n"]
eval:
  # For VLLM this value can be arbitrary large, since it batches the items inside
  batch_size: 800
  context_size_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16284] # [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16284] # 16284 = 16384 - 100
  # max_new_tokens: 100
  log_model_inputs: false